{
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    },
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "<p style=\"text-align:center\">\n    <a href=\"https://skills.network/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML240ENSkillsNetwork34171862-2022-01-01\">\n    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\"  />\n    </a>\n</p>\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "# Linear Regression\n\nEstimated time needed: **45** minutes\n\nIf you are consulting an automobile company, you are trying to understand the factors that influence the sale price of the cars. Specifically, which factors drive the car prices up? And how accurately can you predict the sale price based on the car's features?\n\nIn this notebook, we will perform a simple linear regression analysis on a car price dataset, show how this prediction analysis is done and what are the important assumptions that must be satisfied for linear regression. We will also look at different ways to transform our data.\n\n## Objectives\n\nAfter completing this lab you will be able to:\n\n*   Select the significant features based on the visual analysis\n*   Check the assumptions for Linear Regression model\n*   Apply the Linear Regression model and make the predictions\n*   Apply the pipelines to transform the data\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "***\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## **Setup**\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "For this lab, we will be using the following libraries:\n\n*   [`pandas`](https://pandas.pydata.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML240ENSkillsNetwork34171862-2022-01-01) for managing the data.\n*   [`numpy`](https://numpy.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML240ENSkillsNetwork34171862-2022-01-01) for mathematical operations.\n*   [`seaborn`](https://seaborn.pydata.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML240ENSkillsNetwork34171862-2022-01-01) for visualizing the data.\n*   [`matplotlib`](https://matplotlib.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML240ENSkillsNetwork34171862-2022-01-01) for visualizing the data.\n*   [`sklearn`](https://scikit-learn.org/stable/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML240ENSkillsNetwork34171862-2022-01-01) for machine learning and machine-learning-pipeline related functions.\n*   [`scipy`](https://docs.scipy.org/doc/scipy/tutorial/stats.html/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML240ENSkillsNetwork34171862-2022-01-01) for statistical computations.\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## **Import the required libraries**\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "The following required modules are pre-installed in the Skills Network Labs environment. However if you run this notebook commands in a different Jupyter environment (e.g. Watson Studio or Ananconda) you will need to install these libraries by removing the `#` sign before `!mamba` in the code cell below.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# All Libraries required for this lab are listed below. The libraries pre-installed on Skills Network Labs are commented.\n# !mamba install -qy pandas==1.3.4 numpy==1.21.4 seaborn==0.9.0 matplotlib==3.5.0 scikit-learn==0.20.1\n# Note: If your environment doesn't support \"!mamba install\", use \"!pip install\"",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "#!pip install -U scikit-learn\n\nimport piplite\nawait piplite.install(['tqdm', 'seaborn', 'skillsnetwork', 'pandas', 'numpy', 'scikit-learn'])",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "from tqdm import tqdm\nimport skillsnetwork\nimport numpy as np\nimport pandas as pd\nfrom itertools import accumulate\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.datasets import load_digits, load_wine\nfrom scipy.stats import boxcox\nfrom scipy.stats.mstats import normaltest\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score \nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.pipeline import Pipeline",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "import sklearn; print(\"Scikit-Learn\", sklearn.__version__)",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Surpress warnings:\ndef warn(*args, **kwargs):\n    pass\nimport warnings\nwarnings.warn = warn",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## **Reading and understanding our data**\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "For this lab, we will be using the car sales dataset, hosted on IBM Cloud object storage. The dataset contains all the information about cars, the name of the manufacturer, the year it was launched, all car technical parameters, and the sale price.\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Let's read the data into *pandas* data frame and look at the first 5 rows using the `head()` method.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "URL = 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-ML240EN-SkillsNetwork/labs/data/CarPrice_Assignment.csv'\n\nawait skillsnetwork.download_dataset(URL)\nprint('file downloaded')\n\ndata = pd.read_csv('CarPrice_Assignment.csv')\ndata.head()",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "We can find more information about the features and types using the `info()`  method.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "data.info()",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "According to the output above, we have 205 entries or rows, as well as 26 features. The \"Non-Null Count\" column shows the number of non-null entries.  If the count is 205 then there is no missing values for that particular feature. The 'price' is our target, or response variable, and the rest of the features are our predictor variables.\n\nWe also have a mix of numerical (8 int64 and 8 float64) and object data types (10 object).\n\nThe `describe()` function will provide the statistical information about all numeric values.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "data.describe()",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## **Data Cleaning and Wrangling**\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Here, we will check if we have any missing values.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "data.isnull().sum()",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Also, check for any duplicates by running `duplicated()` function through 'car_ID' records, since each row has a unique car ID value.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "sum(data.duplicated(subset = 'car_ID')) == 0",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Next, let's look into some of our object variables first. Using `unique()` function, we will describe all categories of the 'CarName' attribute.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "data[\"CarName\"].unique()",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "We can see that the 'CarName' includes both the company name (brand) and the car model. Next, we want to split a company name from the model of a car, as for our model building purpose, we will focus on a company name only.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "data['brand'] = data.CarName.str.split(' ').str.get(0).str.lower()",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Let's view all the `unique()` brands now.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "data.brand.unique()",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "There are some typos in the names of the cars, so they should be corrected.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "data['brand'] = data['brand'].replace(['vw', 'vokswagen'], 'volkswagen')\ndata['brand'] = data['brand'].replace(['maxda'], 'mazda')\ndata['brand'] = data['brand'].replace(['porcshce'], 'porsche')\ndata['brand'] = data['brand'].replace(['toyouta'], 'toyota')",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "data.brand.unique()",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Let's plot and sort the total number of Brands.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "fig, ax = plt.subplots(figsize = (15,5))\nplt1 = sns.countplot(x=data['brand'], order=pd.value_counts(data['brand']).index)\nplt1.set(xlabel = 'Brand', ylabel= 'Count of Cars')\nplt.show()\nplt.tight_layout()",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "We can drop 'car_ID', 'symboling', and 'CarName' from our data frame, since they will no longer be needed.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "data.drop(['car_ID', 'symboling', 'CarName'],axis = 1, inplace = True)",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "data.info()",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "#If you need to save this partially processed data, uncomment the line below.\n#data.to_csv('cleaned_car_data.csv', index=False)",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## Exercise 1\n\nIn this exercise, explore any (or all) object variables of your interest.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "data.fueltype.unique() data[\"enginelocation\"].value_counts()",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "<details>\n<summary><strong>Solution</strong> (Click Here)</summary>\n```python\ndata.fueltype.unique()\ndata[\"enginelocation\"].value_counts()\n```\n</details>\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Next, we need to engineer some features, for better visualizations and analysis. We will group our data by 'brand', calculate the average price for each brand, and split these prices into 3 bins: 'Budget', 'Mid-Range', and 'Luxury' cars, naming the newly created column - the 'brand_category'.\n",
      "metadata": {
        "tags": []
      }
    },
    {
      "cell_type": "code",
      "source": "data_comp_avg_price = data[['brand','price']].groupby('brand', as_index = False).mean().rename(columns={'price':'brand_avg_price'})\n",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "data = data.merge(data_comp_avg_price, on = 'brand')",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "We will now check the statistics of our average car price per car brand.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "data.brand_avg_price.describe()",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "data['brand_category'] = data['brand_avg_price'].apply(lambda x : \"Budget\" if x < 10000 \n                                                     else (\"Mid_Range\" if 10000 <= x < 20000\n                                                           else \"Luxury\"))",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## **Exploratory Data Analysis**\n\nList of Categorical Variables:\n\n*   brand_category\n*   fueltype\n*   enginetype\n*   carbody\n*   doornumber\n*   enginelocation\n*   fuelsystem\n*   cylindernumber\n*   aspiration\n*   drivewheel\n\nWe will use the `boxplot()` function on the above mentioned categorical variables, to display the mean, variance, and possible outliers, with respect to the price.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "plt.figure(figsize=(10, 20))\nplt.subplot(4,2,1)\nsns.boxplot(x = 'fueltype', y = 'price', data = data)\nplt.subplot(4,2,2)\nsns.boxplot(x = 'aspiration', y = 'price', data = data)\nplt.subplot(4,2,3)\nsns.boxplot(x = 'carbody', y = 'price', data = data)\nplt.subplot(4,2,4)\nsns.boxplot(x = 'drivewheel', y = 'price', data = data)\nplt.subplot(4,2,5)\nsns.boxplot(x = 'enginetype', y = 'price', data = data)\nplt.subplot(4,2,6)\nsns.boxplot(x = 'brand_category', y = 'price', data = data)\nplt.tight_layout()\nplt.show()",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Next, let's view the list of top features that have high correlation coefficient. The `corr()` function calculates the Pearson's correlation coefficients with respect to the 'price'.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "corr_matrix = data.corr()\ncorr_matrix['price'].sort_values(ascending=False)",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "These are strongly correlated numerical features with Car Price.\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "We can also use the `heatmap()` or `pairplot()` to further explore the relationship between all features and the target variables.\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Exercise 2\n\nUse the `pairplot()` function to display the scatter plots of the relationships between the features.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "sns.pairplot(data) plt.show()",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "<details>\n<summary><strong>Solution</strong> (Click Here)</summary>\n```python\n\nsns.pairplot(data)\nplt.show()\n\n```\n</details>\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## **Testing Assumptions for Linear Regression**\n\nSince we fit a linear model, we assume that the relationship between the target (price) and other features is linear.\n\nWe also expect that the errors, or residuals, are pure random fluctuations around the true line, in other words, the variability in the response (dependent) variable doesn't increase as the value of the predictor (independent) variable increases. This is the assumption of equal variance, also known as *Homoscedasticity*.\n\nWe also assume that the observations are independent of one another (no *multicollinearity*), and there is no correlation between the sequential observations.\n\nIf we see one of these assumptions in the dataset are not met, it's more likely that the other ones, mentioned above, will also be violated. Luckily, we can check and fix these assumptions with a few unique techniques.\n\nNow, let's briefly touch upon each of these assumptions in our example.\n",
      "metadata": {
        "tags": []
      }
    },
    {
      "cell_type": "markdown",
      "source": "### 1. Linearity Assumption\n\nLinear regression needs the relationship between independent variable and the dependent variable to be linear. We can test this assumption with some scatter plots and regression lines.\n\nWe will start with the 'enginesize' and 'horsepower' features.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "fig, (ax1, ax2) = plt.subplots(figsize = (12,8), ncols=2,sharey=False)\nsns.scatterplot( x = data.enginesize, y = data.price,  ax=ax1)\nsns.regplot(x=data.enginesize, y=data.price, ax=ax1)\n \nsns.scatterplot(x = data.horsepower,y = data.price, ax=ax2)\nsns.regplot(x=data.horsepower, y=data.price, ax=ax2);\n",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## Exercise 3\n\nIn this Exercise, plot any other numeric features, using the *seaborn* `regplot()` function, to see whether there is any linear relationship between the feature and the 'price'.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "sns.regplot(x=data.curbweight, y=data.price, data=data)",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "<details>\n<summary><strong>Solution</strong> (Click Here)</summary>\n```python\nsns.regplot(x=data.curbweight, y=data.price, data=data)\n\n```\n</details>\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "### 2. *Homoscedasticity*\n\nThe assumption of *homoscedasticity* (constant variance), is crucial to linear regression models. *Homoscedasticity* describes a situation in which the error term or variance or the \"noise\" or random disturbance in the relationship between the independent variables and the dependent variable is the same across all values of the independent variable. In other words, there is a constant variance present in the response variable as the predictor variable increases. If the \"noise\" is not the same across the values of an independent variable, we call it *heteroscedasticity*, opposite of *homoscedasticity*.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "plt.subplots(figsize = (12,8))\nsns.residplot(x=data[\"enginesize\"], y=data[\"price\"])",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "From the above plot, we can tell the error variance across the true line is dispersed somewhat not uniformly, but in a funnel like shape. So, the assumption of the *homoscedasticity* is more likely not met.\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "### 3. Normality\n\nThe linear regression analysis requires the dependent variable, 'price', to be normally distributed. A histogram, box plot, or a Q-Q-Plot can check if the target variable is normally distributed. The goodness of fit test, e.g., the Kolmogorov-Smirnov test can check for normality in the dependent variable. [This documentation](https://towardsdatascience.com/normality-tests-in-python-31e04aa4f411?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML240ENSkillsNetwork34171862-2022-01-01) contains more information on the normality assumption.\n\nLet's display all three charts to show how our target variable, 'price' behaves.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "def plotting_3_chart(data, feature):\n    ## Importing seaborn, matplotlab and scipy modules. \n    import seaborn as sns\n    import matplotlib.pyplot as plt\n    import matplotlib.gridspec as gridspec\n    from scipy import stats\n    import matplotlib.style as style\n    style.use('fivethirtyeight')\n\n    ## Creating a customized chart. and giving in figsize and everything. \n    fig = plt.figure(constrained_layout=True, figsize=(12,8))\n    ## creating a grid of 3 cols and 3 rows. \n    grid = gridspec.GridSpec(ncols=3, nrows=3, figure=fig)\n    #gs = fig3.add_gridspec(3, 3)\n\n    ## Customizing the histogram grid. \n    ax1 = fig.add_subplot(grid[0, :2])\n    ## Set the title. \n    ax1.set_title('Histogram')\n    ## plot the histogram. \n    sns.distplot(data.loc[:,feature], norm_hist=True, ax = ax1)\n\n    # customizing the QQ_plot. \n    ax2 = fig.add_subplot(grid[1, :2])\n    ## Set the title. \n    ax2.set_title('QQ_plot')\n    ## Plotting the QQ_Plot. \n    stats.probplot(data.loc[:,feature], plot = ax2)\n\n    ## Customizing the Box Plot. \n    ax3 = fig.add_subplot(grid[:, 2])\n    ## Set title. \n    ax3.set_title('Box Plot')\n    ## Plotting the box plot. \n    sns.boxplot(data.loc[:,feature], orient='v', ax = ax3);\n    \nplotting_3_chart(data, 'price')",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "These three charts above can tell us a lot about our target variable:\n\n*   Our target variable, 'price' is not normally distributed\n*   Our target variable is right-skewed\n*   There are some outliers in the variable\n\nThe right-skewed plot means that most prices in the dataset are on the lower end (below 15,000). The 'max' value is very far from the '75%' quantile statistic. All these plots show that the assumption for accurate linear regression modeling is not met.\n\nNext, we will perform the log transformation to correct our target variable and to make it more normally distributed.\n\nBut first, we will save our data that we have changed so far, in the 'previous_data' frame.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "previous_data = data.copy()",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "### Log Transformation\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "We can also check statistically if the target is normally distributed, using `normaltest()` function. If the p-value is large (>0.05), the target variable is normally distributed.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "normaltest(data.price.values)",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "As we can see, the p-value is very small, so it is not normally distributed.\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Now, we can try to transform our data, so it looks more normally distributed. We can use the `np.log()` or `np.log1p`functions from the `numpy` library to perform the log transformation. The `np.log1p` works better with smaller numbers and thus provides more accurate results. This [documentation](https://numpy.org/doc/stable/reference/generated/numpy.log.html?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML240ENSkillsNetwork34171862-2022-01-01) contains more information about the numpy log transform.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "data['price'] = np.log(data['price'])\nplotting_3_chart(data, 'price')",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Let's check our p-value, after the transformation.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "normaltest(data.price.values)",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "As we can see, the log method transformed the car 'price' distribution into a more symmetrical bell curve. It is still not perfect, but it is much closer to being normally distributed.\n\nThere are other ways to correct the skewed data. For example, Square Root Transform (`np.sqrt`) and the Box-Cox Transform (`stats.boxcox` from the `scipy stats` library). To learn more about these two methods, please check out this [article](https://towardsdatascience.com/top-3-methods-for-handling-skewed-data-1334e0debf45?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML240ENSkillsNetwork34171862-2022-01-01).\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Exercise 4\n\nUse the `boxcox()` function to do another transformation on the original, untransformed data (previous_data). Use the `normaltest()` function to check for statistics.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "cp_result = boxcox(previous_data.price) boxcox_price = cp_result[0]\nnormaltest(boxcox_price)",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "<details>\n<summary><strong>Solution</strong> (Click Here)</summary>\n```python\ncp_result = boxcox(previous_data.price)\nboxcox_price = cp_result[0]\n\nnormaltest(boxcox_price)\n\n```\n</details>\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "<details>\n<summary><strong>Answer</strong> (Click Here)</summary>\n    &emsp; &emsp; <code>\nThe higher the p-value is, the closer the distribution is to normal. In our case, pvalue=0.0002332100843764356, is very small, (<0.05), so the target variable is still not normally distributed).\n\n</code>\n</details>\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "### 4. *Multicollinearity*\n\n*Multicollinearity* is when there is a strong correlation between the independent variables. Linear regression or multilinear regression requires independent variables to have little or no similar features. *Multicollinearity* can lead to a variety of problems, including:\n\n*   The effect of predictor variables estimated by our regression will depend on what other variables are included in our model.\n*   Predictors can have widely different results depending on the observations in our sample, and small changes in samples can   result in very different estimated effects.\n*   With very high multicollinearity, the inverse matrix, the computer calculations may not be accurate.\n*   We can no longer interpret a coefficient on a variable because there is no scenario in which one variable can change without a conditional change in another variable.\n\nUsing `heatmap()` function is an excellent way to identify whether there is *multicollinearity* present or not. The best way to solve for *multicollinearity* is to use the regularization methods like *Ridge* or *Lasso*, which we will introduce in the **Regularization** lab.\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Exercise 5\n\nUse the `heatmap()` do display all correlation factors of the numeric variables. Do you see any correlations between the independent features?\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "plt.figure(figsize = (30, 25)) sns.heatmap(data.corr(), annot = True, cmap=\"YlGnBu\") plt.show()",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "<details>\n<summary><strong>Solution</strong> (Click Here)</summary>\n```python\n\nplt.figure(figsize = (30, 25))\nsns.heatmap(data.corr(), annot = True, cmap=\"YlGnBu\")\nplt.show()\n\n```\n</details>\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "<details>\n<summary><strong>Answer</strong> (Click Here)</summary>\n    &emsp; &emsp; <code>\n\nObservation.\nAs we can see, the multicollinearity still exists in various features. However, we will keep them for now for the sake of learning and let the models (e.x. Regularization models such as Lasso, Ridge in the next lab) do the clean up later on.\n\n</code>\n</details>\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## **Linear Regression Model**\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "List of significant variables after Exploratory Data Analysis :\n\nNumerical:\n\n*   Curbweight\n*   Car Length\n*   Car width\n*   Engine Size\n*   Boreratio\n*   Horse Power\n*   Wheel base\n*   City mpg (miles per gallon)\n*   Highway mpg (miles per gallon)\n\nCategorical:\n\n*   Engine Type\n*   Fuel type\n*   Car Body\n*   Aspiration\n*   Cylinder Number\n*   Drivewheel\n*   Brand Category\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "We are going to put all the selected features into a data frame.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "columns=['price', 'fueltype', 'aspiration','carbody', 'drivewheel','wheelbase', 'brand_category',\n                  'curbweight', 'enginetype', 'cylindernumber', 'enginesize', 'boreratio','horsepower', 'carlength','carwidth','citympg','highwaympg']\n\n\n\nselected = data[columns]\nselected.info()",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "We can find the categorical columns by  iterating  through the `dtypes`  attribute.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "categorical_columns=[key for key, value in selected.dtypes.iteritems()  if value=='O']\ncategorical_columns",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## Exercise 6\n\nFind the names of the  numeric columns using the list `columns` and assign them to the list  `numeric_columns`.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "numeric_columns=list(set(columns)-set(categorical_columns)) numeric_columns",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "<details>\n<summary><strong>Solution</strong> (Click Here)</summary>\n```python\n\nnumeric_columns=list(set(columns)-set(categorical_columns))\nnumeric_columns\n\n```\n</details>\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "We can split the data into the features `X` and target `y`.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "X = selected.drop(\"price\", axis=1)\nX.head()\n",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "y = selected[\"price\"].copy()\ny.head()",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Before we used one-hot encoding to deal with the categorical data, let's examine the distribution of the categorical variables:\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "for column in  categorical_columns:\n    print(\"column name:\", column)\n    print(\"value_count:\")\n    print( X[column].value_counts())",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "We see many categorical features have few or one occurrence. For example, we see `three`, `twelve` only occur once in the column `cylindernumber`. Therefore, if the components for the one-hot encoding are constructed using the training data, and the sample in the column `cylindernumber` does not include three or twelve, we will get an error. As a result, we must split the data before the transformation.   This is fine as one-hot encoding is a deterministic transform, but for other transforms, for example standardization, the parameters should be estimated using the training data, then applied to the test data.\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## OneHotEncoder\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "We will use the following modules:\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "from sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "To perform one-hot encoding, we use the `ColumnTransformer` class, this allows different columns or column subsets to be transformed separately.\n\nThe input is as follows:\n\nThe `transformerslist` is the number of tuples.\nThe list of `(name, transformer, columns)` tuples specify the transformer objects to be applied to the subsets of the data.\n\n*   name: name of the operation that can be used later\n*   `transformer`: estimator must support fit and transform, in this case we will use `OneHotEncoder()`\n*   `‘drop’`: to  drop the columns\n*   `‘passthrough’`: to pass them through untransformed data\n*   `remainder`: specifies the columns that are not transformed are being set to `passthrough`. They are  combined in the output, and the non-specified columns are dropped.\n\nWe apply `fit_transform()` to transform the data.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "one_hot = ColumnTransformer(transformers=[(\"one_hot\", OneHotEncoder(), categorical_columns) ],remainder=\"passthrough\")\nX=one_hot.fit_transform(X)\ntype(X)",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "We see the output is a NumPy array, so let's get the feature names from the `one_hot` object using  `get_feature_names_out()` method. The output  will be the feature name with the  prefix of the name of the transformer. For one-hot encoding, the prefix will also include the name of the column that generated that feature.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "names=one_hot.get_feature_names_out()\nnames",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Let's strip out the prefix of the string.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "colunm_names=[name[name.find(\"_\")+1:] for name in  [name[name.find(\"__\")+2:] for name in names]]\ncolunm_names",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "We can save the result as a dataframe to be used in other labs.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "df=pd.DataFrame(data=X,columns=colunm_names)\n#df.to_csv('cleaned_car_data.csv', index=False)",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## Exercise 7\n\nWrite the lines of code  that performs same task as  `ColumnTransformer` using `OneHotEncoder()`.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "X_ = selected[categorical_columns+numeric_columns]\nX_numeric=X[numeric_columns].to_numpy() X_categorical=OneHotEncoder().fit_transform(X_[categorical_columns]).toarray() \nX_=np.concatenate((X_categorical,X_numeric), axis = 1)",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "<details>\n<summary><strong>Solution</strong> (Click Here)</summary>\n```python\n\nX_ = selected[categorical_columns+numeric_columns]\n\nX_numeric=X[numeric_columns].to_numpy()\nX_categorical=OneHotEncoder().fit_transform(X_[categorical_columns]).toarray()\nX_=np.concatenate((X_categorical,X_numeric), axis = 1)\n\n```\n</details>\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Exercise 8\n\nWrite the lines of code that performs same task as  `ColumnTransformer` using `pd.get_dummies`.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "def dummies(x,data):\ntemp = pd.get_dummies(data[x], drop_first = True)\ndata = pd.concat([data, temp], axis = 1)\ndata.drop([x], axis = 1, inplace = True)\nreturn data\nX_ = selected[categorical_columns+numeric_columns]\nN_column=0\nfor column in  categorical_columns:\nprint(pd.unique(data[column]))\nX_ = dummies(column,X_)\n",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "<details>\n<summary><strong>Solution</strong> (Click Here)</summary>\n    &emsp; &emsp; <code>\n\ndef dummies(x,data):\ntemp = pd.get_dummies(data\\[x], drop_first = True)\ndata = pd.concat(\\[data, temp], axis = 1)\ndata.drop(\\[x], axis = 1, inplace = True)\nreturn data\n\nX\\_ = selected\\[categorical_columns+numeric_columns]\nN_column=0\n\nfor column in  categorical_columns:\nprint(pd.unique(data\\[column]))\n\nX\\_ = dummies(column,X\\_)\n\n</code>\n</details>\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Train Test Split\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "We split our data into training and testing sets, using 30% of the data for testing.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "from sklearn.model_selection import train_test_split",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "X_train, X_test, y_train, y_test = train_test_split( df, y, test_size=0.30, random_state=0)",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## Standardize the Data\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "We standardize features by removing the mean and scaling to unit variance using `StandardScaler`, we create a\n`StandardScaler` object:\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "from sklearn.preprocessing import StandardScaler",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "ss=StandardScaler()\nss",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "We `fit` our training data, then we `transform` it.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "X_train=ss.fit_transform(X_train)",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## Linear Regression\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Finally, we apply the `LinearRegression()` model and `fit()` our `X` and `y` data.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "lm = LinearRegression()\nlm.fit(X_train,y_train)\n",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## Making Prediction\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "We will select some random data and apply the `predict()` function.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "X_test=ss.transform(X_test)\ncar_price_predictions = lm.predict(X_test)\ncar_price_predictions",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## Model Evaluation\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Let's evaluate this model with some statistics. We will use *Scikit_Learn's* `mean_squared_error()` function for this evaluation. MSE measures the average of the squares of the errors, that is, the average squared difference between the estimated values and the actual values using the test data. For more information on MSE, please visit this wikipedia [site](https://en.wikipedia.org/wiki/Mean_squared_error?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML240ENSkillsNetwork34171862-2022-01-01).\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "mse = mean_squared_error(y_test, car_price_predictions)\nmse",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Checking the R squared, the coefficient of determination, which is the proportion of the variation in the dependent variable that is predictable from the independent variables. The closer is R squared to 1, the better is the fit of the model.\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "The `score()` method returns the coefficient of determination of the prediction.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "lm.score(X_test,y_test)",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "The `r2_score` method returns the same statistic, also known as the goodness of fit of the model.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "from sklearn.metrics import r2_score ",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "r2_score(y_test,car_price_predictions)",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "If the R squared is negative, it suggests the overfitting, when a statistical model fits exactly against its training data.\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Pipeline Object\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "We can also create a Pipeline object and apply a set of transforms sequentially. Then, we can apply linear regression.  Data Pipelines simplify the steps of processing the data. We use the module Pipeline to create a pipeline. We also use `StandardScaler`as a step in our pipeline.\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "We create the pipeline, by creating a list of tuples including the name of the model or estimator and its corresponding constructor.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "steps=[('scaler', StandardScaler()), ('lm',  LinearRegression())]",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "We input the list as an argument to the pipeline constructor.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "pipe = Pipeline(steps=steps)",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "We `fit` the constructor.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "pipe.fit(X_train,y_train)",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "We make a prediction and perform model evaluation.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "car_price_predictions = pipe.predict(X_test)\nmse = mean_squared_error(y_test, car_price_predictions)\nrmse = np.sqrt(mse)\nrmse\n",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "r2_score(car_price_predictions, y_test)",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## Exercise 9\n\nUse the  `ColumnTransformer` in the pipeline, then train the model using <b>all</b> the data, make a prediction and calculate all the  metrics.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "X = selected[categorical_columns+numeric_columns] \none_hot = ColumnTransformer(transformers=[(\"one_hot\", OneHotEncoder(), categorical_columns) ],remainder=\"passthrough\") steps=[('one_hot',one_hot), ('scaler', StandardScaler()), ('lm', LinearRegression())]\npipe = Pipeline(steps=steps) pipe.fit(X,y) \ncar_price_predictions=pipe.predict(X) \nr2_score(car_price_predictions, y)\n",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "<details>\n<summary><strong>Solution</strong> (Click Here)</summary>\n```python\n\nX = selected[categorical_columns+numeric_columns]\none_hot = ColumnTransformer(transformers=[(\"one_hot\", OneHotEncoder(), categorical_columns) ],remainder=\"passthrough\")\nsteps=[('one_hot',one_hot), ('scaler', StandardScaler()), ('lm',  LinearRegression())]\n\npipe = Pipeline(steps=steps)\npipe.fit(X,y)\ncar_price_predictions=pipe.predict(X)\nr2_score(car_price_predictions, y)\n\n```\n</details>\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "# Congratulations! - You have completed the lab\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Authors\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "[Svitlana Kramar](https://www.linkedin.com/in/svitlana-kramar?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML240ENSkillsNetwork34171862-2022-01-01)\n\n<a href=\"https://www.linkedin.com/in/joseph-s-50398b136/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkML0101ENSkillsNetwork20718538-2021-01-01\">Joseph Santarcangelo</a>\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Change Log\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "| Date (YYYY-MM-DD) | Version | Changed By  | Change Description  |\n| ----------------- | ------- | ----------- | ------------------- |\n| 2022-03-03        | 0.1     | Svitlana K. | Created Initial     |\n| 2022-03-17        | 0.2     | Joseph S.   | Updated all content |\n",
      "metadata": {}
    }
  ]
}