{
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    },
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "<p style=\"text-align:center\">\n    <a href=\"https://skills.network/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML240ENSkillsNetwork34171862-2022-01-01\">\n    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\"  />\n    </a>\n</p>\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "# Cross Validation\n\nEstimated time needed: **45** minutes\n\nWhen performing supervised machine learning analysis, it is common to withhold a portion of the data to test the final model's performance. This model testing is performed on the 'unseen' data, which the model was not trained on. This withholding of a portion of the dataset for testing is called Cross-Validation. Cross-Validation can also be used to select hyper-parameters and test the final model. In this section, we will focus on the test data only.\n\nCross-Validation also helps avoid over-fitting; a complex model could repeat the labels of the samples that it has just seen and, therefore, would have a perfect score but would fail to predict anything useful on the 'unseen' data. Furthermore, a complex model could just be modeling noise.\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Cross validation method involves dividing the dataset into 3 parts:\n\n*   training set - is a portion of the data used for training the model\n*   validation set - is a portion of the data used to optimize the hyper-parameters of the model. This will     be illustrated in the next lab\n*   test set - is a portion of the data used to evaluate if the model generalizes enough to work on the     \n    data it was not trained on   \n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "`Scikit Learn` library contains many methods that can perform the splitting of the data into training, testing and validation sets. The most popular methods that we will cover in this Jupyter Notebook are:\n\n*   train_test_split - creates a single split into train and test sets\n*   K-fold - creates number of k-fold splits, allowing cross validation\n*   cross_val_score - evaluates model's score through cross validation\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Objectives\n\nAfter completing this lab you will be able to:\n\n*   Understand how to split the data into a training and testing sets\n*   Understand and perform K-fold cross validation method\n*   Calculate Cross Validation Scores\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "***\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## **Setup**\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "For this lab, we will be using the following libraries:\n\n*   [`pandas`](https://pandas.pydata.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML240ENSkillsNetwork34171862-2022-01-01) for managing the data.\n*   [`numpy`](https://numpy.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML240ENSkillsNetwork34171862-2022-01-01) for mathematical operations.\n*   [`seaborn`](https://seaborn.pydata.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML240ENSkillsNetwork34171862-2022-01-01) for visualizing the data.\n*   [`matplotlib`](https://matplotlib.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML240ENSkillsNetwork34171862-2022-01-01) for visualizing the data.\n*   [`sklearn`](https://scikit-learn.org/stable/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML240ENSkillsNetwork34171862-2022-01-01) for machine learning and machine-learning-pipeline related functions.\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## **Import the required libraries**\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "The following required modules are pre-installed in the Skills Network Labs environment. However, if you run this notebook commands in a different Jupyter environment (e.g. Watson Studio or Ananconda) you will need to install these libraries by removing the `#` sign before `!mamba` in the code cell below.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# All Libraries required for this lab are listed below. The libraries pre-installed on Skills Network Labs are commented.\n#!mamba install -qy pandas==1.3.4 numpy==1.21.4 seaborn==0.9.0 matplotlib==3.5.0 scikit-learn==0.20.1\n# Note: If your environment doesn't support \"!mamba install\", use \n# \"!pip install pandas==1.3.4 numpy==1.21.4 seaborn==0.9.0 matplotlib==3.5.0 scikit-learn==0.20.1\"",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "#!pip install -U scikit-learn",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Surpress warnings:\ndef warn(*args, **kwargs):\n    pass\nimport warnings\nwarnings.warn = warn",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "import piplite\nawait piplite.install(['tqdm', 'seaborn', 'skillsnetwork', 'pandas', 'numpy', 'scikit-learn'])",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "import pandas as pd\nimport numpy as np \n\nimport seaborn as sns \nimport matplotlib.pylab as plt\n%matplotlib inline\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score \nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.preprocessing import Normalizer\nimport skillsnetwork",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## **Reading our data**\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "For this lab, we will be using the car sales dataset, hosted on IBM Cloud object storage. The dataset contains all the information about cars, the name of the manufacturer, the year it was launched, all car technical parameters, and the sale price. This dataset has already been pre-cleaned and encoded (using one-hot and label encoders) in the Linear Regression Notebook.\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Let's read the data into *pandas* data frame and look at the first 5 rows using the `head()` method.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "URL = 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-ML240EN-SkillsNetwork/labs/encoded_car_data.csv'\n\nawait skillsnetwork.download_dataset(URL)\n\ndata = pd.read_csv('encoded_car_data.csv')\ndata.head()",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "We apply `dtypes.value_counts()` to see what types of data we have.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "data.dtypes.value_counts()",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "We can verify the data type using the method `info()`.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "data.info()",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "As we see from above, we now have only numeric parameters.\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "### Data Preparation\n\nLet's first split our data into `X` features and `y` target.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "X = data.drop(columns=['price'])\ny = data['price'].copy()",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "# Train Test Split\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Now, we split our data into training and testing sets. Training data is used for our model to recognize patterns using some criteria, the test data is used for model evaluation, as shown in the image below.\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "<center>\n    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-ML240EN-SkillsNetwork/images/trin-test.png\">\n</center>\nsource scikit-learn.org\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "We use the function `train_test_split` that  splits arrays or matrices into random train and test subsets. The parameters of the `train_test_split` are:\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "`X,y`: the allowed inputs are lists, numpy arrays, scipy-sparse matrices or pandas data frames.\n\n`test_size`:  If float, it should be between 0.0 and 1.0 and represents the proportion of the dataset to include in the test split. If int (integer), it represents the absolute number of test samples. If None, the value is set to the complement of the train size. If train_size is also None, it will be set to 0.25.\n\nIn our example, we will use 30% of the data for testing and 70% for training.\n\n`random_state:` Controls the shuffling applied to the data before applying the split. Pass an int for reproducible output across multiple function calls. in our case, we set it to \"42\".\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3, random_state=42)",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "### Model Building and Evaluation\n\nLet's perform linear regression using traditional `train_test_split`, which will split the data into train and test set, so that each target value appears in both training and testing sets. We will start by creating a `LinearRegression()` object, lr.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "lr = LinearRegression()",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "We apply the `LinearRegression()` model, m, and `fit()` our `X_train` and `  y_train `  training data.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "lr.fit(X_train, y_train)",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "To make our predictions, we need to use our test data set. We apply `predict()` function on the testing data set.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "predicted =lr.predict(X_test)\n",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Now, let's check some evaluation statistics, such as the coefficient of determination, $R^{2}$, using the built-in method `score` or  `r2_score`, and the Root Mean Square Error, RMSE, for which we can use the `mean_squared_error` method, MSE.\n\nThe $R^{2}$ statistic indicates the percentage of the variance in the dependent variable that the independent variables explain collectively.\n\nRoot Mean Square Error (RMSE) is the standard deviation of the residuals (prediction errors). Residuals are a measure of how far from the regression line data points are; RMSE is a measure of how spread out these residuals are. In other words, it tells you how concentrated the data is around the line of best fit.\n\nFor more information on [R-squared](https://en.wikipedia.org/wiki/Coefficient_of_determination?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML240ENSkillsNetwork34171862-2022-01-01) and [RMSE](https://en.wikipedia.org/wiki/Root-mean-square_deviation?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML240ENSkillsNetwork34171862-2022-01-01), please visit their corresponding documentations.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "lr.score(X_train,y_train)",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Using the training data only, the $R^{2}$ is \\~ 0.93. So, almost 93% of variability in the training data is explained by our model.\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Using the test data  $R^{2}$, we get \\~0.85, not as good as the previous score.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "lr.score(X_test,y_test)",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "We can also use  `r2_score()` method to calculate the $R^2$. It will provide the same result.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "print(r2_score(y_true=y_test, y_pred=predicted))",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Now, let's calculate the RMSE. The smaller the RMSE number the better our model is.\nWe apply `mean_squared_error` to our `y_test`and our predicted data. Then, we take a square root of our MSE, using `np.sqrt()` function.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "mse = mean_squared_error(y_true=y_test, y_pred=predicted)\nrmse = np.sqrt(mse)\nrmse",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "### Prediction Example\n\nLet's select some random data, using `iloc` and see some predicted versus actual values for the car prices.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "some_data = X.iloc[:3]\nsome_labels = y.iloc[:3]",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "print(\"Predictions:\", lr.predict(some_data))",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "print(\"Labels:\", list(some_labels))",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "predicted =lr.predict(X_test)\npredicted",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "We can also use the pipeline to run operations on our data. For example we can standardize our data then perform linear regression by applying the method <code>fit</code>.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "pipe = Pipeline([('ss',StandardScaler() ),('lr', LinearRegression())])\npipe.fit(X_train,y_train)\npipe",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Let's calculate the R squared.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "pipe.score(X_train,y_train)",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Using the training data only, the R squared is \\~ 0.93.\\\nNow, let's check the R squared on the test set.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "pipe.score(X_test,y_test)",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "The R squared is much lower. This value provides more accurate evaluation of our model since we test our model on the 'unseen' data set. In case if the R squared is negative, it is because the model is too complex and the data is overfitting. For more information, please, visit this [documentation](https://en.wikipedia.org/wiki/Overfitting?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML240ENSkillsNetwork34171862-2022-01-01) on overfitting.\n\nThis will make more sense when we explore polynomial regression.\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Exercise 1\n\nCreate a pipeline object called pipe1, replace standardization with normalization. Calculate the $R^{2}$ using the built-in method `score` and for RMSE, using `mean_squared_error` method.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "pipe_1 = Pipeline([('nn',Normalizer() ),('lr', LinearRegression())])\npipe_1.fit(X_train, y_train)\n\n\npipe_1.score(X_train,y_train)\npipe_1.score(X_test,y_test)\n\n\npred =pipe_1.predict(X_test)\n\n\nmse = mean_squared_error(y_true=y_test, y_pred=pred)\nrmse = np.sqrt(mse)\nrmse\n",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "<details>\n<summary><strong>Solution</strong> (Click Here)</summary>\n    &emsp; &emsp; <code>\n\npipe\\_1 = Pipeline(\\[('nn',Normalizer() ),('lr', LinearRegression())])\npipe\\_1.fit(X_train, y_train)\n\npipe\\_1.score(X_train,y_train)\npipe\\_1.score(X_test,y_test)\n\npred =pipe\\_1.predict(X_test)\n\nmse = mean_squared_error(y_true=y_test, y_pred=pred)\nrmse = np.sqrt(mse)\nrmse\n\n</code>\n</details>\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Note, you can also use `normalize` by setting the `LinearRegression(normalize=True)`.\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## One feature\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "We can use the test data to select a feature with the best performance. We have a list of features:\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "features=list(X)\nfeatures",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "We can train a linear regression model using each feature and use the test data to obtain the best feature.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "R_2=[]\npipe = Pipeline([('ss',StandardScaler() ),('lr', LinearRegression())])\n\nfor feature in features:\n    pipe.fit(X_train[[feature]],y_train)\n    R_2.append(pipe.score(X_train[[feature]],y_train))\n    \n    ",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "We can plot the $R^{2}$ for each feature.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "plt.bar(features,R_2)\nplt.xticks(rotation=90)\nplt.ylabel(\"$R^2$\")\nplt.show()",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Now, we select the feature that works best, using `argmax()` function.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "best=features[np.argmax(R_2)]\nbest",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "So, 'enginesize' is the feature that produces the highest $R^{2}$. We then train the feature that works best using all the data.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "pipe.fit(X[[best]],y)",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## Exercise 2\n\nIn this Exercise, find the best feature using the test data, without standardization.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "R_2=[]\n\nfor feature in features:\n\n\nlr.fit(X_train[[feature]], y_train)\nR_2.append(lr.score(X_test[[feature]],y_test))\n\nbest=features[np.argmax(R_2)]\n\n\nplt.bar(features,R_2)\nplt.xticks(rotation=90)\nplt.ylabel(\"$R^2$\")\n\n\nplt.show()\nbest=features[np.argmax(R_2)]\nprint(best) ",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "<details>\n<summary><strong>Solution</strong> (Click Here)</summary>\n    &emsp; &emsp; <code>\nR_2=[]\n\nfor feature in features:\n\n    lr.fit(X_train[[feature]], y_train)\n    R_2.append(lr.score(X_test[[feature]],y_test))\n\nbest=features[np.argmax(R_2)]\n\n\nplt.bar(features,R_2)\nplt.xticks(rotation=90)\nplt.ylabel(\"$R^2$\")\n\n\nplt.show()\nbest=features[np.argmax(R_2)]\nprint(best) \n</code>\n</details>\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "# K Fold Cross Validation\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Cross-validation is a resampling procedure used to evaluate machine learning models on a limited data sample.\n\nThe procedure has a single parameter called k that refers to the number of groups that a given data sample is to be split into. As such, the procedure is often called k-fold cross-validation. When a specific value for k is chosen, it may be used in place of k in the reference to the model, such as k=5 becoming 5-fold cross-validation, as shown in the Diagram below. In this case, we would use K-1 (or 4 folds) for testing a 1 fold for training. K-fold is also used for hyper-parameters selection that we will discuss later.\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "<center>\n    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-ML240EN-SkillsNetwork/images/k-fold.png\">\n</center>\n<center>source scikit-learn.org</center>\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Cross Validation Score\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Now, let's use *Scikit-Learn's* *K-fold cross-validation* method to see whether we can assess the performance of our model. The *K-fold cross-validation* method splits the training set into the number of folds (n_splits), as now in the Diagram above, if we have K folds, K-1 is used for training and one fold is used for testing. The input parameters are as follows:\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "<b>estimatorestimator</b>: The object to use to `fit` the data.\n\n<b>X</b>: array-like of shape (n_samples, n_features). The data to fit. Can be for example a list, or an array.\n\n<b>y</b>: array-like of shape (n_samples,) or (n_samples, n_outputs), default=None. The target variable to try to predict in the case of supervised learning.\n\n<b>scoring</b>: A str or a scorer callable object/ function with signature scorer (estimator, X, y) which should return only a single value.  See model evaluation [documentation](https://scikit-learn.org/stable/modules/model_evaluation.html?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML240ENSkillsNetwork34171862-2022-01-01#scoring-parameter) for more information.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "N=len(X)\nN",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Now, let's create a Linear Regression object.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "lr = LinearRegression()",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Then, calculate cross validation scores based on our testing sets.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "scores = cross_val_score(lr, X, y, scoring =\"r2\", cv=3)",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Now, we have the $R^{2}$ for each fold not used to train the model.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "scores ",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "We can calculate mean and standard deviation using the following function of the `scores`:\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "def display_scores(scores, print_=False):\n    \n    print(\"Scores:\", scores)\n    print(\"Mean:\", scores.mean())\n    print(\"Standard deviation:\", scores.std())\n",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "display_scores(scores)",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "The larger the fold, the better the model performance is, as we are using more samples for training; the variance also decreases.\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Cross Validation Scores are RMSE values for training the data on each of our folds, in our case cv = 3, so we get 3 scores, 1 for each fold.\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Exercise 3\n\nIn this Exercise, compute the cross validation scores for 5 folds, using the linear regression object `lr` and `neg_mean_squared_error` method for scoring.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "scores = cross_val_score(lr, X ,y, scoring =\"neg_mean_squared_error\", cv=5)\nlr_scores = np.sqrt(-scores)\ndisplay_scores(lr_scores)\n",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "<details>\n<summary><strong>Solution</strong> (Click Here)</summary>\n    &emsp; &emsp; <code>\n\nscores = cross_val_score(lr, X ,y, scoring =\"neg_mean_squared_error\", cv=5)\nlr_scores = np.sqrt(-scores)\ndisplay_scores(lr_scores)\n\n</code>\n</details>\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "### K Fold\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "In many cases, we would like to train models that are not available in Scikit-learn or are too large to fit in the memory. We can create a `KFold` object that  Provides train/test indices to split data into train/test sets in an iterative manner.\n\n`n_splitsint`:  A number of folds. Must be at least 2. Changed in version 0.22: n_splits default value changed from 3 to 5.\n\n`shuffle`: Indicates whether to shuffle the data before splitting into batches. Note, the samples within each split will not be shuffled.\n\n`random_state`: the random state.\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "We create the  `KFold` object `kf`, setting the number of splits to 2.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "n_splits=2\nkf = KFold(n_splits = n_splits)",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "We train the model using the `split(X,y)` method. It provides the train/test indices for `X` and `y`. Half the data is used for training in the first iteration, and the rest is used for testing and displaying the indexes for each set.  For the second iteration, the data used for training is used for testing, and the testing data is used for training. We store the $R^2$ for each iteration in the array  `R_2`. The `np.zeros()` function returns a new array of given shape and type, filled with zeros. Then, we calculate the $R^2$ for each of the X_train and X_test splits.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "y = data['price'].copy()\nX = data.drop(columns=['price'])\nR_2 = np.zeros((n_splits,1))\npipe = Pipeline([('ss',StandardScaler() ),('lr', LinearRegression())])\nn=0\nfor k,(train_index, test_index) in enumerate(kf.split(X,y)):\n    print(\"TRAIN:\", train_index)\n    print(\"TEST:\", test_index)\n    X_train, X_test =X.iloc[train_index],X.iloc[test_index]\n    \n    y_train, y_test=y[train_index],y[test_index]\n    pipe.fit(X_train,y_train)\n    n=+1\n    R_2[k]=pipe.score(X_test, y_test)",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "We can calculate the average $R^2$.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "R_2.mean()",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "If we set the number of splits to three, we see 2/3's of the data is used for training.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "n_splits=3\nkf = KFold(n_splits = n_splits)\ny = data['price'].copy()\nX = data.drop(columns=['price'])\nR_2=np.zeros((n_splits,1))\npipe = Pipeline([('ss',StandardScaler() ),('lr', LinearRegression())])\nn=0\nfor k,(train_index, test_index) in enumerate(kf.split(X,y)):\n    print(\"TRAIN:\", train_index)\n    print(\"TEST:\", test_index)\n    X_train, X_test =X.iloc[train_index],X.iloc[test_index]\n    \n    y_train, y_test=y[train_index],y[test_index]\n    pipe.fit(X_train,y_train)\n    n=+1\n    R_2[k]=pipe.score(X_test, y_test)\n    \n    \nR_2.mean()",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## Exercise 4\n\nIt many applications, it is useful to randomly select samples for K fold cross validation. In this Exercise, randomly select samples by setting `shuffle` to `True` in the `KFold` constructor. Use all the parameters, as above.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "n_splits=3\nkf = KFold(n_splits = n_splits,shuffle=True)\ny = data['price'].copy()\nX = data.drop(columns=['price'])\nR_2=np.zeros((n_splits,1))\npipe = Pipeline([('ss',StandardScaler() ),('lr', LinearRegression())])\nn=0\nfor k,(train_index, test_index) in enumerate(kf.split(X,y)):\n    print(\"TRAIN:\", train_index)\n    print(\"TEST:\", test_index)\n    X_train, X_test =X.iloc[train_index],X.iloc[test_index]\n\n\ny_train, y_test=y[train_index],y[test_index]\npipe.fit(X_train,y_train)\nn=+1\nR_2[k]=pipe.score(X_test, y_test)\n\nR_2.mean()\n",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "<details>\n<summary><strong>Solution</strong> (Click Here)</summary>\n    &emsp; &emsp; <code>\n\nn_splits=3\nkf = KFold(n_splits = n_splits,shuffle=True)\ny = data['price'].copy()\nX = data.drop(columns=['price'])\nR_2=np.zeros((n_splits,1))\npipe = Pipeline([('ss',StandardScaler() ),('lr', LinearRegression())])\nn=0\nfor k,(train_index, test_index) in enumerate(kf.split(X,y)):\n    print(\"TRAIN:\", train_index)\n    print(\"TEST:\", test_index)\n    X_train, X_test =X.iloc[train_index],X.iloc[test_index]\n    \n    y_train, y_test=y[train_index],y[test_index]\n    pipe.fit(X_train,y_train)\n    n=+1\n    R_2[k]=pipe.score(X_test, y_test)\n    \n    \nR_2.mean()\n\n</code>\n</details>\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "In the Regularization lab, we will learn how to use cross validation to select hyper-parameters.\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "# Congratulations! - You have completed the lab\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Authors\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "<a href=\"https://www.linkedin.com/in/joseph-s-50398b136/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkML0101ENSkillsNetwork20718538-2021-01-01\">Joseph Santarcangelo</a>\n\n[Svitlana Kramar](https://www.linkedin.com/in/svitlana-kramar?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML240ENSkillsNetwork34171862-2022-01-01)\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Change Log\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "| Date (YYYY-MM-DD) | Version | Changed By  | Change Description             |\n| ----------------- | ------- | ----------- | ------------------------------ |\n| 2022-03-25        | 0.1     | Joseph S.   | Updated all content            |\n| 2022-04-26        | 0.1     | Svitlana K. | Corrected minor grammar errors |\n",
      "metadata": {}
    }
  ]
}